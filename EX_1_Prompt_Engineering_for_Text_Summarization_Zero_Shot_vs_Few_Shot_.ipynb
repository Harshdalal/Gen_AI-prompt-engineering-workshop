{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SwZQ67R3eaz"
      },
      "source": [
        "# Prompt Engineering for Text Summarization: Zero-Shot vs. Few-Shot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3S-DkWuv3ea1"
      },
      "source": [
        "## Introduction\n",
        "**Prompt Engineering** is the art and science of crafting effective inputs (prompts) for Large Language Models (LLMs) to guide their behavior and elicit desired outputs. Instead of training a model from scratch or fine-tuning it, prompt engineering allows us to leverage the vast knowledge embedded within pre-trained LLMs for various tasks, including text summarization.\n",
        "\n",
        "This assignment will focus on two key prompting strategies for summarization:\n",
        "1.  **Zero-Shot Learning:** The LLM is given only the task instruction and the input text. It has no examples of the task in the prompt itself.\n",
        "2.  **Few-Shot Learning:** The LLM is given the task instruction along with a few examples of input-output pairs that demonstrate the desired behavior, before the actual input text for which a summary is required.\n",
        "\n",
        "You will explore how these different approaches influence the quality and characteristics of generated summaries.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEjwy7Yj3ea2"
      },
      "source": [
        "## Learning Objectives\n",
        "Upon completion of this assignment, you should be able to:\n",
        "- Understand the concepts of zero-shot and few-shot prompting.\n",
        "- Design clear and effective zero-shot prompts for abstractive text summarization.\n",
        "- Construct few-shot prompts with illustrative examples to guide LLM behavior.\n",
        "- Experiment with various prompt variations (e.g., length control, tone).\n",
        "- Qualitatively compare the summarization output from different prompting strategies.\n",
        "- Discuss the advantages, disadvantages, and best practices for each prompting approach in summarization.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ey-owFcE3ea3"
      },
      "source": [
        "## Setup and Prerequisites\n",
        "This assignment requires access to a powerful Large Language Model (LLM) through an API (e.g., OpenAI's GPT-3.5/GPT-4, Google's Gemini, Anthropic's Claude) or a robust locally hosted open-source model (e.g., Llama 2, Mistral). You will need to:\n",
        "1.  **Choose an LLM:** Select an LLM you have access to.\n",
        "2.  **Set up API Key/Local Model:** Configure your API key as an environment variable or set up your local model inference environment.\n",
        "3.  **Implement `call_llm_api`:** Fill in the placeholder function below with the actual code to make an API call or run local inference to your chosen LLM. *This is crucial for the assignment to work.*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tPfAxV93ea4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# You might need to install specific client libraries based on your chosen LLM\n",
        "# For example, for OpenAI:\n",
        "# pip install openai\n",
        "# from openai import OpenAI\n",
        "# client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
        "\n",
        "# Or for Google Generative AI (Gemini):\n",
        "# pip install -q -U google-generativeai\n",
        "# import google.generativeai as genai\n",
        "# genai.configure(api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n",
        "# model_gemini = genai.GenerativeModel('gemini-pro')\n",
        "\n",
        "\n",
        "def call_llm_api(prompt: str, model_id: str = \"gpt-3.5-turbo\", max_tokens: int = 150, temperature: float = 0.7) -> str:\n",
        "    \"\"\"\n",
        "    Placeholder function to interact with an LLM API or local model.\n",
        "    YOU MUST REPLACE THIS WITH YOUR ACTUAL LLM CALL.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The input prompt for the LLM.\n",
        "        model_id (str): Identifier for the LLM model (e.g., 'gpt-3.5-turbo', 'gemini-pro').\n",
        "        max_tokens (int): Maximum number of tokens for the generated response.\n",
        "        temperature (float): Controls randomness (higher = more creative).\n",
        "\n",
        "    Returns:\n",
        "        str: The generated text (summary).\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Calling LLM ({model_id}) ---\")\n",
        "    print(\"Prompt (first 200 chars):\\n\", prompt[:200], \"...\")\n",
        "\n",
        "    # --- REPLACE THIS SECTION WITH YOUR ACTUAL LLM INTEGRATION CODE ---\n",
        "    # Example for OpenAI:\n",
        "    # try:\n",
        "    #     response = client.chat.completions.create(\n",
        "    #         model=model_id,\n",
        "    #         messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "    #         max_tokens=max_tokens,\n",
        "    #         temperature=temperature\n",
        "    #     )\n",
        "    #     return response.choices[0].message.content\n",
        "    # except Exception as e:\n",
        "    #     print(f\"Error calling OpenAI API: {e}\")\n",
        "    #     return \"[LLM ERROR: Could not generate summary]\"\n",
        "\n",
        "    # Example for Google Gemini:\n",
        "    # try:\n",
        "    #     response = model_gemini.generate_content(\n",
        "    #         prompt,\n",
        "    #         generation_config=genai.types.GenerationConfig(\n",
        "    #             max_output_tokens=max_tokens,\n",
        "    #             temperature=temperature\n",
        "    #         )\n",
        "    #     )\n",
        "    #     return response.text\n",
        "    # except Exception as e:\n",
        "    #     print(f\"Error calling Gemini API: {e}\")\n",
        "    #     return \"[LLM ERROR: Could not generate summary]\"\n",
        "\n",
        "    # Fallback/Mock response if no API is set up\n",
        "    print(\"\\n--- !!! WARNING: LLM API NOT CONFIGURED !!! ---\")\n",
        "    print(\"    Please replace the placeholder in `call_llm_api` with your actual LLM integration.\")\n",
        "    print(\"    Returning mock summary for demonstration.\")\n",
        "    return f\"[Mock Summary for: {prompt[:50]}...] This is a placeholder. Configure your LLM API!\"\n",
        "\n",
        "\n",
        "# --- Sample Long Texts for Summarization ---\n",
        "sample_texts = [\n",
        "    {\n",
        "        \"id\": \"text_1\",\n",
        "        \"title\": \"The Rise of Quantum Computing\",\n",
        "        \"content\": \"Quantum computing is a rapidly emerging technology that harnesses the principles of quantum mechanics—such as superposition, entanglement, and quantum tunneling—to perform computations. Unlike classical computers that store information as bits (0s or 1s), quantum computers use qubits, which can represent both 0 and 1 simultaneously. This ability allows quantum computers to process vast amounts of information exponentially faster for certain types of problems. Potential applications include breaking modern encryption methods, designing new materials with unprecedented properties, optimizing complex logistical challenges, and accelerating drug discovery. However, the technology is still in its nascent stages, facing significant challenges in qubit stability, error correction, and scalability. Researchers worldwide, including IBM, Google, and various academic institutions, are making considerable progress, yet a fully fault-tolerant quantum computer is likely still decades away.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"text_2\",\n",
        "        \"title\": \"The Impact of AI on Job Markets\",\n",
        "        \"content\": \"Artificial intelligence (AI) is poised to significantly transform global job markets, creating new roles while potentially automating others. Repetitive and routine tasks, whether manual or cognitive, are particularly susceptible to automation. This includes roles in manufacturing, data entry, and certain administrative functions. However, AI is also expected to boost productivity and create entirely new industries and job categories that require human oversight, creativity, emotional intelligence, and critical thinking. Examples include AI ethicists, data scientists, machine learning engineers, and roles focused on human-AI collaboration. The key challenge for societies will be to manage this transition, focusing on reskilling and upskilling programs to prepare the workforce for future demands. Policymakers are also exploring universal basic income (UBI) and other social safety nets to address potential widespread displacement.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"text_3\",\n",
        "        \"title\": \"Reusable Rockets and Space Exploration\",\n",
        "        \"content\": \"The advent of reusable rocket technology has marked a paradigm shift in space exploration, dramatically reducing the cost of launching payloads into orbit. Companies like SpaceX, with its Falcon 9 and Falcon Heavy rockets, have pioneered the vertical landing of first-stage boosters, allowing them to be refuelled and flown multiple times. This innovation contrasts sharply with traditional rockets, which are typically expendable after a single use. The cost savings enable more frequent launches, foster greater accessibility to space, and open up possibilities for ambitious missions like satellite mega-constellations, lunar bases, and eventual human exploration of Mars. While the initial development costs for reusable systems are high, the long-term operational savings are immense, pushing the boundaries of what is economically feasible in space. This has also spurred increased competition and innovation within the aerospace industry.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "print(\"Sample texts loaded. Total texts:\", len(sample_texts))\n",
        "print(\"First sample text (content preview):\\n\", sample_texts[0]['content'][:200], \"...\")\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQFvmtqR3ea7"
      },
      "source": [
        "## Assignment Questions\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaaQ4nv63ea7"
      },
      "source": [
        "### Question 1: Zero-Shot Prompt for Summarization\n",
        "Design a clear and concise zero-shot prompt to summarize `sample_texts[0]['content']` (The Rise of Quantum Computing).\n",
        "\n",
        "1.  **Design Prompt:** Create a zero-shot prompt. Your prompt should clearly instruct the LLM to summarize the provided text. Consider specifying:\n",
        "    * The task: \"Summarize the following article.\"\n",
        "    * Desired output length (e.g., \"in about 3-4 sentences\" or \"in less than 80 words\").\n",
        "    * Any specific format (e.g., \"as a paragraph\").\n",
        "    * The input text itself.\n",
        "2.  **Execute & Print:** Use your `call_llm_api` function to get a summary. Print the original text, the prompt used, and the generated summary.\n",
        "3.  **Qualitative Assessment:** Briefly assess the quality of the summary. Does it capture the main points? Is it coherent and fluent? Does it adhere to your length constraints?\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kgqsp2lT3ea8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQ-QMk0i3ea9"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2SwGdaH3ea9"
      },
      "source": [
        "### Question 2: Few-Shot Prompt for Summarization\n",
        "Now, let's design a few-shot prompt for `sample_texts[1]['content']` (The Impact of AI on Job Markets). Few-shot prompts include examples within the prompt itself to demonstrate the desired summarization style.\n",
        "\n",
        "1.  **Define Examples:** Create 1-2 example pairs of `(original_text, desired_summary)`. These examples should ideally be short and relevant to summarization, but not necessarily from the `sample_texts` list.\n",
        "    * Example 1:\n",
        "        * `Text: \"The capital of France is Paris, a global center for art, fashion, gastronomy, and culture. Its 19th-century cityscape is crisscrossed by wide boulevards and the River Seine. Beyond such landmarks as the Eiffel Tower and the 12th-century Gothic Notre-Dame Cathedral, the city is known for its cafe culture and designer boutiques.\" `\n",
        "        * `Summary: \"Paris, the capital of France, is a global cultural hub known for its art, fashion, and historic landmarks like the Eiffel Tower and Notre-Dame Cathedral, as well as its vibrant cafe culture.\" `\n",
        "    * Example 2 (optional, if you want more): Another similar pair.\n",
        "2.  **Design Prompt:** Construct a few-shot prompt. It should include your general instruction, followed by the example pairs, and then `sample_texts[1]['content']` for summarization.\n",
        "3.  **Execute & Print:** Use your `call_llm_api` function to get a summary. Print the original text, the prompt used, and the generated summary.\n",
        "4.  **Qualitative Assessment and Comparison:** How does this summary compare to the zero-shot summary from Question 1 (in terms of style, detail, adherence to task)? Discuss whether the examples seemed to guide the LLM's output in any observable way.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bgp5x873ea9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVYlKQpu3ea-"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUOU2hYn3ea-"
      },
      "source": [
        "### Question 3: Experimenting with Prompt Variations\n",
        "Prompt engineering allows for fine-grained control over the output. Let's experiment with `sample_texts[2]['content']` (Reusable Rockets).\n",
        "\n",
        "1.  **Variation A (Strict Length Control):** Design a zero-shot prompt that demands a summary of a very specific length, e.g., \"Summarize the following article in *exactly two sentences*.\" or \"Summarize the following article in *under 40 words*.\"\n",
        "    * Execute and print the summary.\n",
        "2.  **Variation B (Persona/Tone):** Design a zero-shot prompt that asks for a summary from a specific persona or tone, e.g., \"Summarize this article as if you are explaining it to a 12-year-old.\" or \"Summarize this article for a busy CEO.\"\n",
        "    * Execute and print the summary.\n",
        "3.  **Discussion:** How well did the LLM adhere to the strict length constraint? Did the persona/tone instruction meaningfully change the summary's style? What challenges did you observe in getting the LLM to follow these specific instructions?\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wuB2ZKiW3ea-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3IS4cTm3ea-"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Km8EkOE3ea_"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcinOMqW3ea_"
      },
      "source": [
        "### Question 4: Comparative Analysis and Best Practices\n",
        "Reflect on your experiments with different prompting strategies.\n",
        "\n",
        "1.  **Zero-Shot vs. Few-Shot:** Based on your experience in Questions 1 and 2, when would you prefer a zero-shot approach, and when would a few-shot approach be more beneficial for text summarization? Provide reasons.\n",
        "2.  **Key Elements of Effective Prompts:** What are the most important elements you discovered for crafting effective prompts for summarization (e.g., clarity of instruction, placement of text, use of examples, specifying constraints)?\n",
        "3.  **Challenges of Prompt Engineering:** Discuss some common challenges or limitations you might face when relying solely on prompt engineering for summarization (e.g., consistency across diverse inputs, ensuring factual accuracy, handling very long documents, dealing with ambiguity, cost of API calls).\n",
        "4.  **When to Fine-tune?** In what scenarios would fine-tuning a model on a custom summarization dataset be a more appropriate strategy than relying on advanced prompting techniques?\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cr3MATX3ea_"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0inwWdQs3ea_"
      },
      "source": [
        "## Submission Guidelines\n",
        "- Ensure your notebook runs without errors from top to bottom (once your `call_llm_api` function is implemented).\n",
        "- Save your notebook as `your_name_prompt_engineering_assignment.ipynb`.\n",
        "- Clearly answer all questions and provide explanations where requested in Markdown cells.\n",
        "- Include the full prompts used for each test case in your code cells.\n",
        "- Feel free to add additional code cells or markdown cells for clarity or experimentation.\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}