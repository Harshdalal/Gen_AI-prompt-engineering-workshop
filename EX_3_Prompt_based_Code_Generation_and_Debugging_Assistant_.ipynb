{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment: Prompt-based Code Generation and Debugging Assistant\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "VYwM8LGEB_fu"
      },
      "id": "VYwM8LGEB_fu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Objective:\n",
        "This assignment aims to explore the capabilities and limitations of large language models (LLMs) in **prompt-based code generation and debugging**. You'll experiment with various prompting strategies to generate code, identify and fix errors, and evaluate the effectiveness of the assistant."
      ],
      "metadata": {
        "id": "yhBc49C7B_fw"
      },
      "id": "yhBc49C7B_fw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instructions:\n",
        "1.  **Environment Setup:** You'll need access to a large language model for this assignment (e.g., Google's Gemini, OpenAI's GPT-4, etc.). If you don't have direct API access, you can use the web interfaces provided by these models.\n",
        "2.  **Jupyter Notebook:** All your work, including **prompts**, **generated code**, **observations**, and **explanations**, should be documented in this Jupyter Notebook.\n",
        "3.  **Code Execution:** For every code snippet generated or debugged, you **must** include a cell that attempts to execute the code and capture any output or errors.\n",
        "4.  **Analysis and Reflection:** Throughout the assignment, you're expected to critically analyze the model's responses, identify patterns, and reflect on the challenges and opportunities of using LLMs for these tasks.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Nn4NhBvQB_fx"
      },
      "id": "Nn4NhBvQB_fx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1: Code Generation\n",
        "In this section, you'll prompt the LLM to generate code for various tasks."
      ],
      "metadata": {
        "id": "h08fiUX6B_fx"
      },
      "id": "h08fiUX6B_fx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task 1.1: Basic Function Generation\n",
        "* **Prompt:**\n",
        "    ```\n",
        "    \"Write a Python function that takes a list of numbers as input and returns the sum of all even numbers in the list. Include a docstring and type hints.\"\n",
        "    ```\n",
        "* **Steps:**\n",
        "    1.  Paste the prompt into the LLM.\n",
        "    2.  Copy the generated code and paste it into the code cell below.\n",
        "    3.  Execute the code with a few test cases (e.g., `sum_even_numbers([1, 2, 3, 4, 5, 6])`, `sum_even_numbers([])`, `sum_even_numbers([2, 4, 6])`).\n",
        "* **Analysis:**\n",
        "    * Did the code correctly implement the logic?\n",
        "    * Were the docstring and type hints included and accurate?\n",
        "    * Note any discrepancies or unexpected behaviors."
      ],
      "metadata": {
        "id": "BvRzQg6vB_fy"
      },
      "id": "BvRzQg6vB_fy"
    },
    {
      "cell_type": "code",
      "source": [
        "# Paste generated code here\n",
        "# Example test cases:\n",
        "# print(sum_even_numbers([1, 2, 3, 4, 5, 6]))\n",
        "# print(sum_even_numbers([]))\n",
        "# print(sum_even_numbers([2, 4, 6]))"
      ],
      "metadata": {
        "id": "fO8EC1XLB_fy"
      },
      "outputs": [],
      "id": "fO8EC1XLB_fy",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task 1.2: Intermediate Algorithm Generation\n",
        "* **Prompt:**\n",
        "    ```\n",
        "    \"Generate a C++ program that implements the Merge Sort algorithm for an array of integers. The program should include a `main` function to test the sort with a sample array and print the sorted output.\"\n",
        "    ```\n",
        "* **Steps:**\n",
        "    1.  Paste the prompt into the LLM.\n",
        "    2.  Copy the generated code and paste it into the code cell below.\n",
        "    3.  Execute the code with the sample array.\n",
        "* **Analysis:**\n",
        "    * Is the Merge Sort implementation correct?\n",
        "    * Are there any common errors in C++ memory management or syntax?\n",
        "    * How readable is the generated code?"
      ],
      "metadata": {
        "id": "3BDCboFxB_fz"
      },
      "id": "3BDCboFxB_fz"
    },
    {
      "cell_type": "code",
      "source": [
        "# Paste generated C++ code here\n",
        "# You might need to use %%bash or other magic commands if you're not using a C++ kernel.\n",
        "# Example: %%bash\n",
        "# g++ your_code.cpp -o your_program\n",
        "# ./your_program"
      ],
      "metadata": {
        "id": "allWtpeNB_fz"
      },
      "outputs": [],
      "id": "allWtpeNB_fz",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task 1.3: Code Generation with Specific Libraries/APIs\n",
        "* **Prompt:**\n",
        "    ```\n",
        "    \"Using Python's `requests` library, write a script that fetches data from '[https://jsonplaceholder.typicode.com/posts/1](https://jsonplaceholder.typicode.com/posts/1)', parses the JSON response, and prints the 'title' and 'body' fields. Handle potential network errors gracefully.\"\n",
        "    ```\n",
        "* **Steps:**\n",
        "    1.  Paste the prompt into the LLM.\n",
        "    2.  Copy the generated code and paste it into the code cell below.\n",
        "    3.  Execute the code.\n",
        "* **Analysis:**\n",
        "    * Did the code correctly use the `requests` library?\n",
        "    * Was error handling implemented effectively?\n",
        "    * Did it correctly parse the JSON and extract the requested fields?"
      ],
      "metadata": {
        "id": "Y-YPxyVGB_fz"
      },
      "id": "Y-YPxyVGB_fz"
    },
    {
      "cell_type": "code",
      "source": [
        "# Paste generated code here\n",
        "# import requests # Ensure requests is installed: pip install requests"
      ],
      "metadata": {
        "id": "OJA4YH4SB_fz"
      },
      "outputs": [],
      "id": "OJA4YH4SB_fz",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "tsfug4cLB_fz"
      },
      "id": "tsfug4cLB_fz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 2: Code Debugging\n",
        "In this section, you'll present buggy code to the LLM and ask it to identify and fix the errors."
      ],
      "metadata": {
        "id": "ItVN5rfQB_f0"
      },
      "id": "ItVN5rfQB_f0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task 2.1: Semantic Error Debugging\n",
        "* **Buggy Code (Python):**\n",
        "    ```python\n",
        "    def calculate_average(numbers):\n",
        "        total = 0\n",
        "        for num in numbers:\n",
        "            total += num\n",
        "        return total / len(numbers) if len(numbers) > 0 else 0\n",
        "\n",
        "    data = [1, 2, 3, 4, 5, 0]\n",
        "    print(calculate_average(data))\n",
        "    ```\n",
        "* **Prompt to LLM:**\n",
        "    ```\n",
        "    \"The following Python code is supposed to calculate the average of a list of numbers, but it sometimes produces incorrect results. Identify the semantic error and provide the corrected code. Explain your reasoning.\"\n",
        "    ```\n",
        "    (Provide the buggy code along with the prompt.)\n",
        "* **Steps:**\n",
        "    1.  Copy the buggy code and the prompt into the LLM.\n",
        "    2.  Paste the LLM's identified error and corrected code into new cells.\n",
        "    3.  Execute the corrected code with the `data` example and other edge cases (e.g., empty list, list with negative numbers).\n",
        "* **Analysis:**\n",
        "    * Did the LLM correctly identify the semantic error?\n",
        "    * Was the explanation clear and accurate?\n",
        "    * Is the corrected code robust?"
      ],
      "metadata": {
        "id": "lY9zS-KVB_f0"
      },
      "id": "lY9zS-KVB_f0"
    },
    {
      "cell_type": "code",
      "source": [
        "# Original buggy code (for reference)\n",
        "def calculate_average(numbers):\n",
        "    total = 0\n",
        "    for num in numbers:\n",
        "        total += num\n",
        "    return total / len(numbers) if len(numbers) > 0 else 0\n",
        "\n",
        "data = [1, 2, 3, 4, 5, 0]\n",
        "print(f\"Original code output: {calculate_average(data)}\")\n"
      ],
      "metadata": {
        "id": "2Gk3c4s_B_f0"
      },
      "outputs": [],
      "id": "2Gk3c4s_B_f0",
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Paste LLM's corrected code here\n",
        "# Example test cases:\n",
        "# print(corrected_calculate_average(data))\n",
        "# print(corrected_calculate_average([]))\n",
        "# print(corrected_calculate_average([10, 20, 30]))"
      ],
      "metadata": {
        "id": "1W1RXNMkB_f0"
      },
      "outputs": [],
      "id": "1W1RXNMkB_f0",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task 2.2: Syntax Error Debugging\n",
        "* **Buggy Code (JavaScript):**\n",
        "    ```javascript\n",
        "    function greet(name) {\n",
        "        console.log('Hello, ' + name + '!');\n",
        "    }\n",
        "\n",
        "    greet(\"Alice\"\n",
        "    ```\n",
        "* **Prompt to LLM:**\n",
        "    ```\n",
        "    \"The following JavaScript code has a syntax error. Please identify the error and provide the corrected code. Explain what was wrong.\"\n",
        "    ```\n",
        "    (Provide the buggy code along with the prompt.)\n",
        "* **Steps:**\n",
        "    1.  Copy the buggy code and the prompt into the LLM.\n",
        "    2.  Paste the LLM's identified error and corrected code into new cells.\n",
        "    3.  Execute the corrected code (e.g., in a browser's console or Node.js environment, or using `%%javascript` magic in Jupyter).\n",
        "* **Analysis:**\n",
        "    * Did the LLM quickly and accurately pinpoint the syntax error?\n",
        "    * Was the explanation helpful for a beginner?"
      ],
      "metadata": {
        "id": "BZEUumqMB_f0"
      },
      "id": "BZEUumqMB_f0"
    },
    {
      "cell_type": "code",
      "source": [
        "# Original buggy code (for reference)\n",
        "%%javascript\n",
        "function greet(name) {\n",
        "    console.log('Hello, ' + name + '!');\n",
        "}\n",
        "\n",
        "greet(\"Alice\""
      ],
      "metadata": {
        "id": "4CVWluSnB_f1"
      },
      "outputs": [],
      "id": "4CVWluSnB_f1",
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Paste LLM's corrected code here\n",
        "%%javascript\n",
        "# Example:\n",
        "# function correctedGreet(name) {\n",
        "#     console.log('Hello, ' + name + '!');\n",
        "# }\n",
        "# correctedGreet(\"Bob\");"
      ],
      "metadata": {
        "id": "E5O8jJOMB_f1"
      },
      "outputs": [],
      "id": "E5O8jJOMB_f1",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task 2.3: Logic Error Debugging\n",
        "* **Buggy Code (Python):**\n",
        "    ```python\n",
        "    def factorial(n):\n",
        "        if n == 0:\n",
        "            return 1\n",
        "        else:\n",
        "            result = n\n",
        "            for i in range(n): # Incorrect loop range\n",
        "                result *= i\n",
        "            return result\n",
        "\n",
        "    print(factorial(5)) # Expected: 120\n",
        "    ```\n",
        "* **Prompt to LLM:**\n",
        "    ```\n",
        "    \"The following Python function for calculating factorial produces incorrect results for positive integers. Identify and fix the logical error. Provide the corrected code and explain the mistake.\"\n",
        "    ```\n",
        "    (Provide the buggy code along with the prompt.)\n",
        "* **Steps:**\n",
        "    1.  Copy the buggy code and the prompt into the LLM.\n",
        "    2.  Paste the LLM's identified error and corrected code into new cells.\n",
        "    3.  Execute the corrected code with `factorial(5)` and other values (e.g., `factorial(0)`, `factorial(1)`, `factorial(3)`).\n",
        "* **Analysis:**\n",
        "    * Did the LLM correctly identify the logical flaw?\n",
        "    * Was the explanation of the logical error clear?\n",
        "    * Is the corrected code efficient and correct?"
      ],
      "metadata": {
        "id": "GdS9MzGsB_f1"
      },
      "id": "GdS9MzGsB_f1"
    },
    {
      "cell_type": "code",
      "source": [
        "# Original buggy code (for reference)\n",
        "def factorial(n):\n",
        "    if n == 0:\n",
        "        return 1\n",
        "    else:\n",
        "        result = n\n",
        "        for i in range(n): # Incorrect loop range\n",
        "            result *= i\n",
        "        return result\n",
        "\n",
        "print(f\"Original code output for factorial(5): {factorial(5)}\")\n"
      ],
      "metadata": {
        "id": "4cxGtmthB_f1"
      },
      "outputs": [],
      "id": "4cxGtmthB_f1",
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Paste LLM's corrected code here\n",
        "# Example test cases:\n",
        "# print(corrected_factorial(5))\n",
        "# print(corrected_factorial(0))\n",
        "# print(corrected_factorial(1))"
      ],
      "metadata": {
        "id": "TidX4M2FB_f1"
      },
      "outputs": [],
      "id": "TidX4M2FB_f1",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "KNJazjLJB_f2"
      },
      "id": "KNJazjLJB_f2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 3: Advanced Scenarios and Evaluation"
      ],
      "metadata": {
        "id": "aVIQHfFvB_f2"
      },
      "id": "aVIQHfFvB_f2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task 3.1: Multi-turn Interaction for Refinement\n",
        "* **Scenario:** You want to generate a Python script that reads a CSV file, filters rows based on a specific column value, and then writes the filtered data to a new CSV file.\n",
        "* **Steps:**\n",
        "    1.  **Initial Prompt:** Start with a high-level prompt like: \"Write a Python script to filter a CSV file based on a column value.\"\n",
        "    2.  **Refinement 1:** Based on the LLM's response, ask for specific improvements, eg., \"Now, modify the script to allow the user to specify the input and output filenames, the column name, and the filter value as command-line arguments using `argparse`.\"\n",
        "    3.  **Refinement 2 (Error Introduction & Debugging):** Intentionally introduce a small bug in the LLM's generated code (e.g., a typo in an argument name, or incorrect type conversion) and then ask the LLM to debug it.\n",
        "    4.  **Documentation:** Document each prompt, the LLM's response, and your observations at each step. Include code execution results.\n",
        "* **Analysis:**\n",
        "    * How well did the LLM handle multi-turn interactions?\n",
        "    * Was it able to incorporate new requirements effectively?\n",
        "    * How did it perform when debugging code it partially generated?"
      ],
      "metadata": {
        "id": "V3PqOqmuB_f2"
      },
      "id": "V3PqOqmuB_f2"
    },
    {
      "cell_type": "code",
      "source": [
        "# Document your multi-turn interactions and code here. Add new cells as needed.\n",
        "# Initial Prompt Response:\n",
        "# [Paste initial code here]\n",
        "\n",
        "# Refinement 1 Prompt Response:\n",
        "# [Paste refined code here]\n",
        "\n",
        "# Refinement 2 (Debugging) Prompt Response:\n",
        "# [Paste debugged code here]"
      ],
      "metadata": {
        "id": "FfTuiEVwB_f2"
      },
      "outputs": [],
      "id": "FfTuiEVwB_f2",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task 3.2: Limitations and Biases\n",
        "* **Experiment:** Design a prompt that you anticipate might lead to a less optimal or potentially biased response from the LLM when generating code or debugging. (e.g., a very niche programming language, a task with ethical implications, or a task requiring highly specific domain knowledge).\n",
        "* **Prompt:** [Your chosen prompt]\n",
        "* **Steps:**\n",
        "    1.  Execute your chosen prompt with the LLM.\n",
        "    2.  Analyze the generated code or debugging response.\n",
        "* **Analysis:**\n",
        "    * What limitations or biases did you observe?\n",
        "    * How did the LLM's performance degrade in this scenario?\n",
        "    * Suggest ways to mitigate these limitations or biases in future prompt engineering."
      ],
      "metadata": {
        "id": "Zw7YOQAGB_f2"
      },
      "id": "Zw7YOQAGB_f2"
    },
    {
      "cell_type": "code",
      "source": [
        "# Document your chosen prompt and LLM's response here.\n",
        "# [Paste LLM's response/code here]"
      ],
      "metadata": {
        "id": "idlrSdzNB_f2"
      },
      "outputs": [],
      "id": "idlrSdzNB_f2",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "WelatdiHB_f2"
      },
      "id": "WelatdiHB_f2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 4: Conclusion and Reflection\n",
        "In this markdown cell, provide a comprehensive summary of your findings and reflections based on this assignment.\n",
        "\n",
        "* **Summary of Strengths:** What are the key strengths of prompt-based code generation and debugging assistants?\n",
        "* **Summary of Weaknesses:** What are their primary weaknesses and limitations?\n",
        "* **Best Practices for Prompting:** Based on your experience, what are some best practices for crafting effective prompts for code generation and debugging?\n",
        "* **Future of LLMs in Software Development:** How do you envision LLMs impacting the future of software development and debugging?\n",
        "* **Ethical Considerations:** What ethical considerations arise when relying on AI for code generation and debugging?"
      ],
      "metadata": {
        "id": "TMAHVd2DB_f3"
      },
      "id": "TMAHVd2DB_f3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "iF-RB5QXB_f3"
      },
      "id": "iF-RB5QXB_f3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Submission:\n",
        "* Ensure all code cells have been executed and their outputs are visible.\n",
        "* All analysis and reflections are clearly written in markdown cells.\n",
        "* Save your Jupyter Notebook as `[YourName]_CodeGen_Debug_Assignment.ipynb`."
      ],
      "metadata": {
        "id": "qOOEAmmmB_f3"
      },
      "id": "qOOEAmmmB_f3"
    }
  ]
}