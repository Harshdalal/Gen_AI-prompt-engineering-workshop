{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Patterns for Multimodal Caption Generation\n",
        "\n",
        "This assignment explores how different **prompt patterns** influence the quality and style of **multimodal captions** generated by large language models (LLMs). You'll experiment with various strategies to create diverse and accurate descriptions for images and videos.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "UdSL_q6MC1G6"
      },
      "id": "UdSL_q6MC1G6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objective\n",
        "The goal of this assignment is to understand and apply prompt engineering techniques for **multimodal caption generation**. You'll learn to:\n",
        "* Design prompts that elicit specific caption styles (e.g., descriptive, creative, factual).\n",
        "* Analyze the impact of prompt patterns on the content and quality of generated captions.\n",
        "* Evaluate the effectiveness of different patterns for diverse multimodal inputs."
      ],
      "metadata": {
        "id": "9-QeGd83C1G8"
      },
      "id": "9-QeGd83C1G8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "3cFonS2_C1G9"
      },
      "id": "3cFonS2_C1G9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instructions\n",
        "1.  **Environment Setup**: You'll need access to a large language model with multimodal capabilities (e.g., Google's Gemini, OpenAI's GPT-4o, LLaVA). If you don't have direct API access, use the web interfaces for these models.\n",
        "2.  **Jupyter Notebook**: All your work, including **prompts**, **multimodal inputs (references to images/videos)**, **generated captions**, **observations**, and **analysis**, must be documented in this Jupyter Notebook.\n",
        "3.  **Multimodal Inputs**: For each task, select 2-3 diverse images or short video clips. These should vary in content, complexity, and emotional tone. Provide links to publicly accessible images/videos or clearly describe them if local.\n",
        "4.  **Analysis and Reflection**: Critically analyze the model's responses, identify patterns in caption generation, and reflect on the strengths and weaknesses of different prompt patterns."
      ],
      "metadata": {
        "id": "A3Lo4IrWC1G9"
      },
      "id": "A3Lo4IrWC1G9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "CmRHloHEC1G-"
      },
      "id": "CmRHloHEC1G-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Basic Captioning with Standard Prompts\n",
        "In this section, you'll start with straightforward prompts to generate basic descriptions."
      ],
      "metadata": {
        "id": "5DyCbtNaC1G-"
      },
      "id": "5DyCbtNaC1G-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1.1: Simple Descriptive Caption\n",
        "* **Prompt Pattern**: Direct instruction.\n",
        "* **Prompt**: \"Generate a short, descriptive caption for this image/video.\"\n",
        "* **Steps**:\n",
        "    1.  Choose **2-3 distinct multimodal inputs**.\n",
        "    2.  For each input, use the prompt and record the generated caption.\n",
        "* **Analysis**:\n",
        "    * How factual and objective are the captions?\n",
        "    * Do they capture the main subjects and actions accurately?\n",
        "    * What level of detail is provided without further instruction?"
      ],
      "metadata": {
        "id": "l3GxNwVDC1G_"
      },
      "id": "l3GxNwVDC1G_"
    },
    {
      "cell_type": "code",
      "source": [
        "# Input 1: [Link to image/video or description]\n",
        "# Prompt: \"Generate a short, descriptive caption for this image/video.\"\n",
        "# Generated Caption:\n",
        "\n",
        "# Input 2: [Link to image/video or description]\n",
        "# Prompt: \"Generate a short, descriptive caption for this image/video.\"\n",
        "# Generated Caption:\n",
        "\n",
        "# Input 3: [Link to image/video or description]\n",
        "# Prompt: \"Generate a short, descriptive caption for this image/video.\"\n",
        "# Generated Caption:\n",
        "\n",
        "# Analysis for Task 1.1:"
      ],
      "metadata": {
        "id": "pJFVB23BC1G_"
      },
      "outputs": [],
      "id": "pJFVB23BC1G_",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1.2: Factual Captioning\n",
        "* **Prompt Pattern**: Emphasizing facts and details.\n",
        "* **Prompt**: \"Provide a factual and objective caption for this image/video. Focus on verifiable elements and avoid interpretation.\"\n",
        "* **Steps**:\n",
        "    1.  Use the **same 2-3 multimodal inputs** from Task 1.1.\n",
        "    2.  For each input, use the new prompt and record the generated caption.\n",
        "* **Analysis**:\n",
        "    * How do these captions differ from Task 1.1?\n",
        "    * Are they more precise? Do they omit subjective language?\n",
        "    * Did the model successfully avoid interpretation?"
      ],
      "metadata": {
        "id": "kO6E1f6AC1HA"
      },
      "id": "kO6E1f6AC1HA"
    },
    {
      "cell_type": "code",
      "source": [
        "# Input 1: [Link to image/video or description]\n",
        "# Prompt: \"Provide a factual and objective caption for this image/video. Focus on verifiable elements and avoid interpretation.\"\n",
        "# Generated Caption:\n",
        "\n",
        "# Input 2: [Link to image/video or description]\n",
        "# Prompt: \"Provide a factual and objective caption for this image/video. Focus on verifiable elements and avoid interpretation.\"\n",
        "# Generated Caption:\n",
        "\n",
        "# Input 3: [Link to image/video or description]\n",
        "# Prompt: \"Provide a factual and objective caption for this image/video. Focus on verifiable elements and avoid interpretation.\"\n",
        "# Generated Caption:\n",
        "\n",
        "# Analysis for Task 1.2:"
      ],
      "metadata": {
        "id": "cyeE63h7C1HB"
      },
      "outputs": [],
      "id": "cyeE63h7C1HB",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "Aq3SNGfZC1HB"
      },
      "id": "Aq3SNGfZC1HB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Advanced Prompt Patterns for Creative and Styled Captions\n",
        "This section explores how to steer caption generation towards specific styles or tones."
      ],
      "metadata": {
        "id": "19rYqdGEC1HC"
      },
      "id": "19rYqdGEC1HC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2.1: Creative/Evocative Caption\n",
        "* **Prompt Pattern**: Persona-driven or stylistic instruction.\n",
        "* **Prompt**: \"Imagine you are a poet/storyteller. Generate a creative and evocative caption for this image/video, focusing on mood and atmosphere.\"\n",
        "* **Steps**:\n",
        "    1.  Choose **2-3 new, distinct multimodal inputs** (perhaps more artistic or emotionally rich ones).\n",
        "    2.  For each input, use the prompt and record the generated caption.\n",
        "* **Analysis**:\n",
        "    * How did the persona/style instruction influence the caption's tone and vocabulary?\n",
        "    * Did the captions successfully convey mood and atmosphere?\n",
        "    * Are there any instances where the creativity overshadowed accuracy?"
      ],
      "metadata": {
        "id": "mwRStmpcC1HC"
      },
      "id": "mwRStmpcC1HC"
    },
    {
      "cell_type": "code",
      "source": [
        "# Input 1: [Link to image/video or description]\n",
        "# Prompt: \"Imagine you are a poet/storyteller. Generate a creative and evocative caption for this image/video, focusing on mood and atmosphere.\"\n",
        "# Generated Caption:\n",
        "\n",
        "# Input 2: [Link to image/video or description]\n",
        "# Prompt: \"Imagine you are a poet/storyteller. Generate a creative and evocative caption for this image/video, focusing on mood and atmosphere.\"\n",
        "# Generated Caption:\n",
        "\n",
        "# Analysis for Task 2.1:"
      ],
      "metadata": {
        "id": "NBddRmR1C1HC"
      },
      "outputs": [],
      "id": "NBddRmR1C1HC",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2.2: Contextual/Scenario-Based Caption\n",
        "* **Prompt Pattern**: Providing a scenario or specific context for the caption.\n",
        "* **Prompt**: \"Generate a caption for this image/video as if it were for a news report/social media post/travel blog. Consider the intended audience and platform.\"\n",
        "* **Steps**:\n",
        "    1.  Use the **same 2-3 inputs** from Task 2.1.\n",
        "    2.  For each input, try at least **two different contexts** (e.g., \"news report\" and \"social media post\"). Record the captions.\n",
        "* **Analysis**:\n",
        "    * How did the context (news vs. social media) change the caption's length, formality, and content?\n",
        "    * Did the model adapt to the implied audience and platform requirements?\n",
        "    * Which context was the model most effective at addressing?"
      ],
      "metadata": {
        "id": "A-CkngkNC1HD"
      },
      "id": "A-CkngkNC1HD"
    },
    {
      "cell_type": "code",
      "source": [
        "# Input 1: [Link to image/video or description]\n",
        "# Prompt (News Report): \"Generate a caption for this image/video as if it were for a news report. Consider the intended audience and platform.\"\n",
        "# Generated Caption (News Report):\n",
        "\n",
        "# Prompt (Social Media Post): \"Generate a caption for this image/video as if it were for a social media post. Consider the intended audience and platform.\"\n",
        "# Generated Caption (Social Media Post):\n",
        "\n",
        "# Analysis for Task 2.2:"
      ],
      "metadata": {
        "id": "hYiGTGEHC1HD"
      },
      "outputs": [],
      "id": "hYiGTGEHC1HD",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2.3: Constraint-Based Captioning\n",
        "* **Prompt Pattern**: Imposing specific constraints (e.g., length, keywords, sentiment).\n",
        "* **Prompt**: \"Generate a concise caption for this image/video, exactly 10 words long, and include the word 'vibrant'.\"\n",
        "* **Steps**:\n",
        "    1.  Choose **2-3 inputs**.\n",
        "    2.  For each input, apply **different constraints** (e.g., 5 words, specific keyword, positive sentiment, question format). Record the captions.\n",
        "* **Analysis**:\n",
        "    * How well did the model adhere to the specified constraints (word count, keywords, sentiment)?\n",
        "    * Did adhering to constraints impact the overall quality or naturalness of the caption?\n",
        "    * Which types of constraints were easier for the model to follow?"
      ],
      "metadata": {
        "id": "MqWnS3LaC1HD"
      },
      "id": "MqWnS3LaC1HD"
    },
    {
      "cell_type": "code",
      "source": [
        "# Input 1: [Link to image/video or description]\n",
        "# Prompt (10 words, 'vibrant'): \"Generate a concise caption for this image/video, exactly 10 words long, and include the word 'vibrant'.\"\n",
        "# Generated Caption:\n",
        "\n",
        "# Prompt (5 words, positive sentiment): \"Generate a caption for this image/video, exactly 5 words long, with a positive sentiment.\"\n",
        "# Generated Caption:\n",
        "\n",
        "# Analysis for Task 2.3:"
      ],
      "metadata": {
        "id": "2N4sK-t_C1HE"
      },
      "outputs": [],
      "id": "2N4sK-t_C1HE",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "82Av8tpeC1HE"
      },
      "id": "82Av8tpeC1HE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Iterative Prompt Refinement and Multimodal Grounding\n",
        "This section focuses on refining prompts and understanding how well the LLM grounds captions in visual details."
      ],
      "metadata": {
        "id": "sxDWWq-2C1HE"
      },
      "id": "sxDWWq-2C1HE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 3.1: Iterative Refinement for Specificity\n",
        "* **Scenario**: You want a very detailed and specific caption for a complex image/video.\n",
        "* **Steps**:\n",
        "    1.  Select **one complex multimodal input** (e.g., a busy scene, a detailed landscape, an event).\n",
        "    2.  **Initial Prompt**: Start with a general prompt (e.g., \"Describe this image/video in detail.\").\n",
        "    3.  **Refinement 1**: Based on the initial caption, ask follow-up questions or add instructions to elicit more specific details (e.g., \"Focus on the objects in the background,\" or \"Describe the interaction between the two people.\").\n",
        "    4.  **Refinement 2**: Continue refining based on the model's response, pushing for even greater detail or specific perspectives.\n",
        "    5.  Document each prompt, the generated caption, and your observations at each step.\n",
        "* **Analysis**:\n",
        "    * How effective is iterative prompting in achieving highly specific captions?\n",
        "    * At what point does the model's ability to extract new information diminish?\n",
        "    * Did the model maintain coherence across turns?"
      ],
      "metadata": {
        "id": "JnjgIAHfC1HE"
      },
      "id": "JnjgIAHfC1HE"
    },
    {
      "cell_type": "code",
      "source": [
        "# Input: [Link to complex image/video or description]\n",
        "\n",
        "# Initial Prompt:\n",
        "# Generated Caption (Initial):\n",
        "\n",
        "# Refinement 1 Prompt:\n",
        "# Generated Caption (Refinement 1):\n",
        "\n",
        "# Refinement 2 Prompt:\n",
        "# Generated Caption (Refinement 2):\n",
        "\n",
        "# Analysis for Task 3.1:"
      ],
      "metadata": {
        "id": "nUzc0-diC1HF"
      },
      "outputs": [],
      "id": "nUzc0-diC1HF",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 3.2: Error Detection and Correction (Simulated)\n",
        "* **Scenario**: The LLM might occasionally miss details or hallucinate.\n",
        "* **Steps**:\n",
        "    1.  Choose **1 multimodal input**.\n",
        "    2.  Generate an initial caption for it.\n",
        "    3.  **Manually edit this generated caption** to introduce a subtle error or omission (e.g., misidentify a color, omit a prominent object, state something not present).\n",
        "    4.  **Prompt to LLM**: Present the image/video *and* your edited (buggy) caption. Ask: \"Review this caption for accuracy based on the provided image/video. If there are inaccuracies, please correct them and explain why.\"\n",
        "    5.  Record the LLM's correction and explanation.\n",
        "* **Analysis**:\n",
        "    * How effectively did the LLM detect the intentionally introduced error?\n",
        "    * Was its correction accurate and its explanation clear?\n",
        "    * What does this suggest about the model's grounding capabilities?"
      ],
      "metadata": {
        "id": "UpXaAsFbC1HF"
      },
      "id": "UpXaAsFbC1HF"
    },
    {
      "cell_type": "code",
      "source": [
        "# Input: [Link to image/video or description]\n",
        "\n",
        "# Initial Generated Caption:\n",
        "\n",
        "# Manually Edited (Buggy) Caption:\n",
        "\n",
        "# Prompt to LLM:\n",
        "# Corrected Caption from LLM:\n",
        "# LLM's Explanation:\n",
        "\n",
        "# Analysis for Task 3.2:"
      ],
      "metadata": {
        "id": "0xSWMon_C1HF"
      },
      "outputs": [],
      "id": "0xSWMon_C1HF",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "HsKdaajKC1HG"
      },
      "id": "HsKdaajKC1HG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Conclusion and Reflection\n",
        "In this markdown cell, provide a comprehensive summary of your findings and reflections based on this assignment.\n",
        "\n",
        "* **Effectiveness of Prompt Patterns**: Which prompt patterns were most effective for different types of captions (descriptive, creative, constrained)? Why?\n",
        "* **Challenges and Limitations**: What were the main challenges you faced? What limitations of current multimodal LLMs did you observe regarding caption generation?\n",
        "* **Best Practices**: Based on your experiments, what are your top 3-5 best practices for designing effective prompts for multimodal captioning?\n",
        "* **Future Directions**: How do you envision prompt engineering evolving for multimodal tasks? What potential applications or research areas excite you?\n",
        "* **Ethical Considerations**: What ethical concerns (e.g., bias, misrepresentation, privacy) might arise from the widespread use of AI-generated captions?"
      ],
      "metadata": {
        "id": "mKoG9gXNC1HG"
      },
      "id": "mKoG9gXNC1HG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "Xd0s72TRC1HG"
      },
      "id": "Xd0s72TRC1HG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submission\n",
        "* Ensure all code cells (if any, though mostly markdown/analysis) have been executed and their outputs/observations are clearly documented.\n",
        "* All analysis and reflections are clearly written in markdown cells.\n",
        "* Save your Jupyter Notebook as `[YourName]_MultimodalCaptioning_Assignment.ipynb`."
      ],
      "metadata": {
        "id": "CVS9w3sXC1HG"
      },
      "id": "CVS9w3sXC1HG"
    }
  ]
}